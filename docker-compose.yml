services:
    postgres:
        image: postgres:15
        environment:
            POSTGRES_DB: ragplatform
            POSTGRES_USER: postgres
            POSTGRES_PASSWORD: postgres
        ports:
            - "5433:5432"
        volumes:
            - postgres_data:/var/lib/postgresql/data
        healthcheck:
            test: [ "CMD-SHELL", "pg_isready -U postgres" ]
            interval: 10s
            timeout: 5s
            retries: 5

    redis:
        image: redis:7-alpine
        ports:
            - "6379:6379"
        volumes:
            - redis_data:/data
        command: redis-server --appendonly yes
        healthcheck:
            test: [ "CMD", "redis-cli", "ping" ]
            interval: 10s
            timeout: 5s
            retries: 5

    qdrant:
        image: qdrant/qdrant:latest
        ports:
            - "6333:6333"
            - "6334:6334"
        volumes:
            - qdrant_data:/qdrant/storage
        environment:
            QDRANT__SERVICE__GRPC_PORT: 6334
        healthcheck:
            test: [ "CMD-SHELL", "timeout 2 bash -c '</dev/tcp/localhost/6333' || exit 1" ]
            interval: 10s
            timeout: 5s
            retries: 5

    # Optional: Jaeger for tracing (ingestion pipeline observability)
    jaeger:
        image: jaegertracing/all-in-one:latest
        ports:
            - "16686:16686" # UI
            - "4317:4317" # OTLP gRPC
        environment:
            COLLECTOR_OTLP_ENABLED: "true"
        profiles:
            - observability

    litellm:
        image: ghcr.io/berriai/litellm:main-latest
        ports:
            - "4000:4000"
        volumes:
            - ./litellm.yaml:/app/config.yaml:ro
        command: ["--config", "/app/config.yaml", "--port", "4000"]
        extra_hosts:
            - "spark-1.internal:${SPARK_1_IP:-192.168.1.72}"
            - "spark-2.internal:${SPARK_2_IP:-192.168.1.49}"
        restart: on-failure
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
            interval: 15s
            timeout: 5s
            retries: 5
            start_period: 10s

    backend:
        build:
            context: .
            dockerfile: backend/Dockerfile
        ports:
            - "3000:3000"
        env_file:
            - .env
        extra_hosts:
            - "spark-1.internal:${SPARK_1_IP:-192.168.1.72}"
            - "spark-2.internal:${SPARK_2_IP:-192.168.1.49}"
        environment:
            PORT: 3000
            # Database
            DB_HOST: postgres
            DB_PORT: 5432
            DB_NAME: ragplatform
            DB_USER: postgres
            DB_PASSWORD: postgres
            # Redis (must match transcription-worker for jobs to be picked up)
            REDIS_HOST: redis
            REDIS_PORT: 6379
            REDIS_DB: "0"
            # Qdrant
            QDRANT_HOST: qdrant
            QDRANT_PORT: 6333
            # Security
            SECRET_KEY: ${SECRET_KEY:-your-super-secret-key-change-in-production-min-32-chars}
            # LLM/Embedding
            LLM_API_BASE: http://litellm:4000/v1
            LLM_API_KEY: sk-dummy-key
            EMBEDDING_API_BASE: http://litellm:4000/v1
            EMBEDDING_API_KEY: sk-dummy-key
            EMBEDDING_MODEL: ${EMBEDDING_MODEL:-embeddings}
            EMBEDDING_DIMENSION: ${EMBEDDING_DIMENSION:-1024}
            # Document processing
            MARKER_USE_LLM: ${MARKER_USE_LLM:-false}
            UPLOAD_DIR: /app/uploads
            # STT/Whisper — routed via spark-9965's LiteLLM (stt service is internal to that node)
            WHISPER_API_BASE: http://spark-1.internal:4000/v1
            WHISPER_API_KEY: sk-dummy-key
            STT_MODEL: stt
            # TTS — same reasoning
            TTS_API_BASE: http://spark-1.internal:4000/v1
            TTS_API_KEY: sk-dummy-key
            TTS_MODEL: tts
            # Telemetry (optional)
            OTEL_ENABLED: ${OTEL_ENABLED:-false}
            OTEL_EXPORTER_OTLP_ENDPOINT: http://jaeger:4317
        depends_on:
            postgres:
                condition: service_healthy
            redis:
                condition: service_healthy
            qdrant:
                condition: service_healthy
            litellm:
                condition: service_healthy
        volumes:
            - uploads_data:/app/uploads
            # Mount jobs directory to host for easy file access
            - ./jobs:/app/uploads/jobs

    # Unified Worker - single queue (arq:queue), dispatches to transcription + ingestion handlers
    # Built from worker.Dockerfile (includes crawl4ai, docling, opencv); backend image stays lean.
    worker:
        build:
            context: .
            dockerfile: worker.Dockerfile
        command: [ "arq", "app.workers.unified.WorkerSettings" ]
        working_dir: /app
        env_file:
            - .env
        extra_hosts:
            - "spark-1.internal:${SPARK_1_IP:-192.168.1.72}"
            - "spark-2.internal:${SPARK_2_IP:-192.168.1.49}"
        environment:
            # Database
            DB_HOST: postgres
            DB_PORT: 5432
            DB_NAME: ragplatform
            DB_USER: postgres
            DB_PASSWORD: postgres
            # Redis (must match backend: same host, port, db)
            REDIS_HOST: redis
            REDIS_PORT: 6379
            REDIS_DB: "0"
            # STT/Whisper — routed via spark-9965's LiteLLM
            WHISPER_API_BASE: http://spark-1.internal:4000/v1
            WHISPER_API_KEY: sk-dummy-key
            STT_MODEL: stt
            # LLM (for subtitle translation)
            LLM_API_BASE: http://litellm:4000/v1
            LLM_API_KEY: sk-dummy-key
            # Storage
            UPLOAD_DIR: /app/uploads
            # Email (optional)
            EMAIL_ENABLED: ${EMAIL_ENABLED:-false}
            SMTP_HOST: ${SMTP_HOST:-}
            SMTP_PORT: ${SMTP_PORT:-587}
            SMTP_USER: ${SMTP_USER:-}
            SMTP_PASSWORD: ${SMTP_PASSWORD:-}
            SMTP_FROM: ${SMTP_FROM:-}
            FRONTEND_BASE_URL: ${FRONTEND_BASE_URL:-http://localhost:5174}
            # Ingestion (for ingestion_* handlers)
            QDRANT_HOST: qdrant
            QDRANT_PORT: 6333
            # Embeddings via aladin LiteLLM
            EMBEDDING_API_BASE: http://litellm:4000/v1
            EMBEDDING_API_KEY: sk-dummy-key
            EMBEDDING_MODEL: ${EMBEDDING_MODEL:-embeddings}
            EMBEDDING_DIMENSION: ${EMBEDDING_DIMENSION:-1024}
            EMBEDDING_MAX_INPUT_CHARS: ${EMBEDDING_MAX_INPUT_CHARS:-1024}
            # Docling and VLM via aladin LiteLLM
            DOCLING_API_BASE: http://litellm:4000/v1
            DOCLING_MODEL: ${DOCLING_MODEL:-granite_docling}
            VLM_API_BASE: http://litellm:4000/v1
            VLM_API_KEY: sk-dummy-key
            VLM_MODEL: ${VLM_MODEL:-cosmos}
            # YOLO/Roboflow Inference API (video object tracking)
            YOLO_API_URL: ${YOLO_API_URL:-}
            YOLO_API_KEY: ${YOLO_API_KEY:-}
            YOLO_MODEL_ID: ${YOLO_MODEL_ID:-yolov8n-640/1}
            # VLM outcome logging (video ingestion review)
            LOG_VLM_OUTCOME: ${LOG_VLM_OUTCOME:-false}
            LOG_VLM_OUTCOME_SAMPLE_EVERY: ${LOG_VLM_OUTCOME_SAMPLE_EVERY:-1}
            LOG_VLM_OUTCOME_REVIEW_FILE: ${LOG_VLM_OUTCOME_REVIEW_FILE:-false}
            # CV pipeline debug: per-frame images + detections JSON at ./jobs/cv_debug on host
            LOG_CV_DEBUG: ${LOG_CV_DEBUG:-false}
            CV_DEBUG_OUTPUT_DIR: ${CV_DEBUG_OUTPUT_DIR:-/app/uploads/jobs/cv_debug}
            # OCR tuning: multi-scale, CLAHE, unsharp; lower thresh = more sensitive
            OCR_MAX_SIDES: ${OCR_MAX_SIDES:-2200,3000}
            OCR_MIN_CONF: ${OCR_MIN_CONF:-0.55}
            OCR_MERGE_IOU: ${OCR_MERGE_IOU:-0.35}
            OCR_KEEP_TOPK: ${OCR_KEEP_TOPK:-200}
            OCR_USE_ANGLE_CLS: ${OCR_USE_ANGLE_CLS:-true}
            OCR_CLAHE_CLIP_LIMIT: ${OCR_CLAHE_CLIP_LIMIT:-2.5}
            OCR_UNSHARP_SIGMA: ${OCR_UNSHARP_SIGMA:-1.1}
            OCR_UNSHARP_AMOUNT: ${OCR_UNSHARP_AMOUNT:-0.9}
            OCR_DET_DB_THRESH: ${OCR_DET_DB_THRESH:-0.25}
            OCR_DET_DB_BOX_THRESH: ${OCR_DET_DB_BOX_THRESH:-0.55}
            OCR_USE_CLAHE: ${OCR_USE_CLAHE:-true}
            OCR_DET_LIMIT_SIDE_LEN: ${OCR_DET_LIMIT_SIDE_LEN:-1920}
            # YOLO imgsz: 1280 = better for 4K, 640 = faster (default)
            CV_IMGSZ: ${CV_IMGSZ:-1280}
            WORKER_CONCURRENCY: ${WORKER_CONCURRENCY:-5}
            JOB_TIMEOUT: ${JOB_TIMEOUT:-600}
            CHUNK_SIZE: ${CHUNK_SIZE:-1000}
            CHUNK_OVERLAP: ${CHUNK_OVERLAP:-200}
            TEMP_DIR: /app/temp
            OTEL_ENABLED: ${OTEL_ENABLED:-false}
            OTEL_EXPORTER_OTLP_ENDPOINT: http://jaeger:4317
            PYTHONPATH: /app:/app/worker
        depends_on:
            postgres:
                condition: service_healthy
            redis:
                condition: service_healthy
            qdrant:
                condition: service_healthy
            litellm:
                condition: service_healthy
        volumes:
            - uploads_data:/app/uploads
            - ./jobs:/app/uploads/jobs
            - ./worker:/app/worker:ro

    frontend:
        build:
            context: ./frontend
            dockerfile: Dockerfile
            args:
                VITE_API_URL: /api
        ports:
            - "5174:80"
        depends_on:
            backend:
                condition: service_started
        restart: on-failure

    chat-ui:
        build:
            context: ./chat-ui
            dockerfile: Dockerfile
        ports:
            - "7860:80"
        depends_on:
            backend:
                condition: service_started
        restart: on-failure

volumes:
    postgres_data:
    redis_data:
    qdrant_data:
    uploads_data:
