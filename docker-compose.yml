services:
    postgres:
        image: postgres:15
        environment:
            POSTGRES_DB: ragplatform
            POSTGRES_USER: postgres
            POSTGRES_PASSWORD: postgres
        ports:
            - "5433:5432"
        volumes:
            - postgres_data:/var/lib/postgresql/data
        healthcheck:
            test: [ "CMD-SHELL", "pg_isready -U postgres" ]
            interval: 10s
            timeout: 5s
            retries: 5

    redis:
        image: redis:7-alpine
        ports:
            - "6379:6379"
        volumes:
            - redis_data:/data
        command: redis-server --appendonly yes
        healthcheck:
            test: [ "CMD", "redis-cli", "ping" ]
            interval: 10s
            timeout: 5s
            retries: 5

    qdrant:
        image: qdrant/qdrant:latest
        ports:
            - "6333:6333"
            - "6334:6334"
        volumes:
            - qdrant_data:/qdrant/storage
        environment:
            QDRANT__SERVICE__GRPC_PORT: 6334
        healthcheck:
            test: [ "CMD-SHELL", "timeout 2 bash -c '</dev/tcp/localhost/6333' || exit 1" ]
            interval: 10s
            timeout: 5s
            retries: 5

    # Optional: Jaeger for tracing (ingestion pipeline observability)
    jaeger:
        image: jaegertracing/all-in-one:latest
        ports:
            - "16686:16686" # UI
            - "4317:4317" # OTLP gRPC
        environment:
            COLLECTOR_OTLP_ENABLED: "true"
        profiles:
            - observability

    backend:
        build:
            context: .
            dockerfile: backend/Dockerfile
        ports:
            - "3000:3000"
        env_file:
            - .env
        extra_hosts:
            - "llm-host.local:${LLM_HOST_IP:-192.168.1.72}"
        environment:
            PORT: 3000
            # Database
            DB_HOST: postgres
            DB_PORT: 5432
            DB_NAME: ragplatform
            DB_USER: postgres
            DB_PASSWORD: postgres
            # Redis (must match transcription-worker for jobs to be picked up)
            REDIS_HOST: redis
            REDIS_PORT: 6379
            REDIS_DB: "0"
            # Qdrant
            QDRANT_HOST: qdrant
            QDRANT_PORT: 6333
            # Security
            SECRET_KEY: ${SECRET_KEY:-your-super-secret-key-change-in-production-min-32-chars}
            # LLM/Embedding
            LLM_API_BASE: ${LLM_API_BASE:-http://llm-host.local:4000/v1}
            LLM_API_KEY: ${LLM_API_KEY:-sk-dummy-key}
            EMBEDDING_API_BASE: ${EMBEDDING_API_BASE:-http://llm-host.local:4000/v1}
            EMBEDDING_API_KEY: ${EMBEDDING_API_KEY:-sk-dummy-key}
            EMBEDDING_MODEL: ${EMBEDDING_MODEL:-text-embedding-3-small}
            EMBEDDING_DIMENSION: ${EMBEDDING_DIMENSION:-1536}
            # Document processing
            MARKER_USE_LLM: ${MARKER_USE_LLM:-false}
            UPLOAD_DIR: /app/uploads
            # Whisper API (optional - for dedicated transcription service)
            WHISPER_API_BASE: ${WHISPER_API_BASE:-}
            WHISPER_API_KEY: ${WHISPER_API_KEY:-}
            WHISPER_MODEL_SIZE: ${WHISPER_MODEL_SIZE:-base}
            # Telemetry (optional)
            OTEL_ENABLED: ${OTEL_ENABLED:-false}
            OTEL_EXPORTER_OTLP_ENDPOINT: http://jaeger:4317
        depends_on:
            postgres:
                condition: service_healthy
            redis:
                condition: service_healthy
            qdrant:
                condition: service_healthy
        volumes:
            - uploads_data:/app/uploads
            # Mount jobs directory to host for easy file access
            - ./jobs:/app/uploads/jobs

    # Unified Worker - single queue (arq:queue), dispatches to transcription + ingestion handlers
    # Built from worker.Dockerfile (includes crawl4ai, docling, opencv); backend image stays lean.
    worker:
        build:
            context: .
            dockerfile: worker.Dockerfile
        command: [ "arq", "app.workers.unified.WorkerSettings" ]
        working_dir: /app
        env_file:
            - .env
        extra_hosts:
            - "llm-host.local:${LLM_HOST_IP:-192.168.1.72}"
        environment:
            # Database
            DB_HOST: postgres
            DB_PORT: 5432
            DB_NAME: ragplatform
            DB_USER: postgres
            DB_PASSWORD: postgres
            # Redis (must match backend: same host, port, db)
            REDIS_HOST: redis
            REDIS_PORT: 6379
            REDIS_DB: "0"
            # Whisper API (transcription)
            WHISPER_API_BASE: ${WHISPER_API_BASE:-}
            WHISPER_API_KEY: ${WHISPER_API_KEY:-}
            WHISPER_MODEL_SIZE: ${WHISPER_MODEL_SIZE:-base}
            # LLM (for subtitle translation)
            LLM_API_BASE: ${LLM_API_BASE:-http://llm-host.local:4000/v1}
            LLM_API_KEY: ${LLM_API_KEY:-sk-dummy-key}
            # Storage
            UPLOAD_DIR: /app/uploads
            # Email (optional)
            EMAIL_ENABLED: ${EMAIL_ENABLED:-false}
            SMTP_HOST: ${SMTP_HOST:-}
            SMTP_PORT: ${SMTP_PORT:-587}
            SMTP_USER: ${SMTP_USER:-}
            SMTP_PASSWORD: ${SMTP_PASSWORD:-}
            SMTP_FROM: ${SMTP_FROM:-}
            FRONTEND_BASE_URL: ${FRONTEND_BASE_URL:-http://localhost:5174}
            # Ingestion (for ingestion_* handlers)
            QDRANT_HOST: qdrant
            QDRANT_PORT: 6333
            # Embeddings: same LiteLLM endpoint as LLM (e.g. llm-host.local:4000/v1).
            EMBEDDING_API_BASE: ${EMBEDDING_API_BASE:-http://llm-host.local:4000/v1}
            EMBEDDING_API_KEY: ${EMBEDDING_API_KEY:-sk-dummy-key}
            EMBEDDING_MODEL: ${EMBEDDING_MODEL:-text-embedding-3-small}
            EMBEDDING_DIMENSION: ${EMBEDDING_DIMENSION:-1536}
            EMBEDDING_MAX_INPUT_CHARS: ${EMBEDDING_MAX_INPUT_CHARS:-1024}
            # External Docling (optional). Base URL for document conversion; model name via DOCLING_MODEL (e.g. granite_docling).
            DOCLING_API_BASE: ${DOCLING_API_BASE:-}
            DOCLING_MODEL: ${DOCLING_MODEL:-granite_docling}
            VLM_API_BASE: ${VLM_API_BASE:-}
            VLM_API_KEY: ${VLM_API_KEY:-}
            VLM_MODEL: ${VLM_MODEL:-}
            # YOLO/Roboflow Inference API (video object tracking)
            YOLO_API_URL: ${YOLO_API_URL:-}
            YOLO_API_KEY: ${YOLO_API_KEY:-}
            YOLO_MODEL_ID: ${YOLO_MODEL_ID:-yolov8n-640/1}
            # VLM outcome logging (video ingestion review)
            LOG_VLM_OUTCOME: ${LOG_VLM_OUTCOME:-false}
            LOG_VLM_OUTCOME_SAMPLE_EVERY: ${LOG_VLM_OUTCOME_SAMPLE_EVERY:-1}
            LOG_VLM_OUTCOME_REVIEW_FILE: ${LOG_VLM_OUTCOME_REVIEW_FILE:-false}
            WORKER_CONCURRENCY: ${WORKER_CONCURRENCY:-5}
            JOB_TIMEOUT: ${JOB_TIMEOUT:-600}
            CHUNK_SIZE: ${CHUNK_SIZE:-1000}
            CHUNK_OVERLAP: ${CHUNK_OVERLAP:-200}
            TEMP_DIR: /app/temp
            OTEL_ENABLED: ${OTEL_ENABLED:-false}
            OTEL_EXPORTER_OTLP_ENDPOINT: http://jaeger:4317
            PYTHONPATH: /app:/app/worker
        depends_on:
            postgres:
                condition: service_healthy
            redis:
                condition: service_healthy
            qdrant:
                condition: service_healthy
        volumes:
            - uploads_data:/app/uploads
            - ./jobs:/app/uploads/jobs
            - ./worker:/app/worker:ro

    frontend:
        build:
            context: ./frontend
            dockerfile: Dockerfile
            args:
                VITE_API_URL: /api
        ports:
            - "5174:80"
        depends_on:
            backend:
                condition: service_started
        restart: on-failure

    chat-ui:
        build:
            context: ./chat-ui
            dockerfile: Dockerfile
        ports:
            - "7860:80"
        depends_on:
            backend:
                condition: service_started
        restart: on-failure

volumes:
    postgres_data:
    redis_data:
    qdrant_data:
    uploads_data:
