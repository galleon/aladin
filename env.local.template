# ===========================================
# RAG Agent Platform - Environment Configuration
# ===========================================
# Copy this file to .env and adjust values as needed

# ----- Database -----
DB_HOST=postgres
DB_PORT=5432
DB_NAME=ragplatform
DB_USER=postgres
DB_PASSWORD=postgres

# ----- Authentication -----
# Generate a secure key: openssl rand -hex 32
SECRET_KEY=your-super-secret-key-change-in-production-min-32-chars
# JWT lifetime in minutes. If you see "Signature has expired" / 401, log in again or increase (e.g. 10080 = 7 days)
ACCESS_TOKEN_EXPIRE_MINUTES=1440

# ----- Qdrant Vector Store -----
QDRANT_HOST=qdrant
QDRANT_PORT=6333
# QDRANT_API_KEY=  # Uncomment if using Qdrant Cloud

# ----- LLM API (OpenAI-compatible endpoint) -----
# IP for llm-host.local (containers resolve this hostname via docker-compose extra_hosts).
# Change this to the IP of your LLM server.
LLM_HOST_IP=192.168.1.72
# Models are fetched dynamically from this endpoint via /v1/models
LLM_API_BASE=http://llm-host:8000/v1
LLM_API_KEY=sk-dummy-key-for-local-api

# ----- Embedding API (OpenAI-compatible endpoint) -----
# With LiteLLM proxy, use the model_name from litellm.yaml (e.g. embeddings for openai/bge-large-en-v1.5).
EMBEDDING_API_BASE=http://llm-host:8000/v1
EMBEDDING_API_KEY=sk-dummy-key-for-local-api
EMBEDDING_MODEL=embeddings
# Max chars per chunk sent to embedding API (e.g. 2048 ≈ 512 tokens for bge; avoid ContextWindowExceededError)
# Max chars per chunk (512-token models need ~1536 or less to avoid ContextWindowExceededError).
# EMBEDDING_MAX_INPUT_CHARS=1536

# ----- External Docling API (optional, for file ingestion worker) -----
# Document conversion only (granite_docling). Use a different URL from EMBEDDING_API_BASE (e.g. Docling on 4000, embeddings on 8000).
# DOCLING_API_BASE=http://llm-host.local:4000/v1
# DOCLING_MODEL=granite_docling

# ----- File Uploads -----
UPLOAD_DIR=/app/uploads
MAX_FILE_SIZE=52428800

# ----- Marker PDF Extraction -----
# Set to true to use the LLM API for improved PDF extraction quality
# Default is false (uses marker's built-in models only)
MARKER_USE_LLM=false

# ----- RAG Settings -----
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# ----- Video Transcription Settings (REQUIRED) -----
# External Whisper API endpoint - REQUIRED for video transcription agents
# Video agent creation will be disabled if not set
#
# Option 1: Use OpenAI's Whisper API
# WHISPER_API_BASE=https://api.openai.com/v1
# WHISPER_API_KEY=sk-your-openai-api-key
#
# Option 2: Use custom Whisper service (must implement /transcribe endpoint)
# Example: http://whisper-host:8001 or http://192.168.1.100:8001
# WHISPER_API_BASE=http://your-whisper-host:8001
# WHISPER_API_KEY=optional-api-key-if-needed

# ----- VLM API (video ingestion / data domains) -----
# Vision model for video segment analysis. Used as default in Data Domain UI.
# VLM_API_BASE=http://llm-host:4000/v1
# VLM_API_KEY=sk-dummy-key
# VLM_MODEL=gpt-4o

# Prompt library: path to JSON file with keyword -> template (e.g. procedure, race). Optional; default is backend/config/video_prompts.json
# VIDEO_PROMPT_LIBRARY_PATH=/app/config/video_prompts.json

# ----- YOLO/Roboflow Inference API (video object tracking) -----
# When YOLO_API_URL and YOLO_API_KEY are set, object_tracker=yolo uses the API instead of local YOLO.
# object_tracker=yolo_api always uses the API (requires these vars).
# Hosted: https://detect.roboflow.com | Self-hosted: http://inference:9001
# YOLO_API_URL=https://detect.roboflow.com
# YOLO_API_KEY=your-roboflow-api-key
# YOLO_MODEL_ID=yolov8n-640/1

# ----- Video ingestion: VLM outcome logging (worker) -----
# Set to true to log VLM model output per segment for review
# LOG_VLM_OUTCOME=false
# Log every Nth segment (1 = all, 5 = every 5th) to limit volume
# LOG_VLM_OUTCOME_SAMPLE_EVERY=1
# When true, write vlm_out to sidecar .vlm_review.jsonl file for file-based review
# LOG_VLM_OUTCOME_REVIEW_FILE=false

# ----- Video ingestion: CV pipeline debug (worker) -----
# When true, write per-frame images and detections JSON to disk (visible at ./jobs/cv_debug on host)
# LOG_CV_DEBUG=false
# CV_DEBUG_OUTPUT_DIR=/app/uploads/jobs/cv_debug

# ----- OCR tuning (PaddleOCR) - improve detection on video frames -----
# Multi-scale: comma-separated max side lengths; "2200,3000" = quality, "1920" = faster
# OCR_MAX_SIDES=2200,3000
# Min confidence per detection (0.55 = filter noise)
# OCR_MIN_CONF=0.55
# IoU threshold for merging overlapping detections across scales
# OCR_MERGE_IOU=0.35
# Cap on detections per frame
# OCR_KEEP_TOPK=200
# Angle classifier for rotated text (adds ~50-100ms per frame)
# OCR_USE_ANGLE_CLS=true
# det_db_thresh: 0.25 = slightly more sensitive than PaddleOCR default 0.3
# OCR_DET_DB_THRESH=0.25
# det_db_box_thresh: 0.55 = keep slightly more boxes than default 0.6
# OCR_DET_DB_BOX_THRESH=0.55
# det_limit_side_len: max side before resize; 1920=4K→1080p
# OCR_DET_LIMIT_SIDE_LEN=1920
# Upscale small frames (1=off, 2=2x) - helps small overlay text
# OCR_UPSCALE_FACTOR=2
# CLAHE contrast enhancement; clip_limit 2.5 = stronger than default 2.0
# OCR_USE_CLAHE=true
# OCR_CLAHE_CLIP_LIMIT=2.5
# Unsharp mask: sigma, amount (0=disabled)
# OCR_UNSHARP_SIGMA=1.1
# OCR_UNSHARP_AMOUNT=0.9

# ----- Object detection (YOLO) - input size for 4K video -----
# CV_IMGSZ: 1280 = better for 4K, 640 = faster (YOLO default)
# CV_IMGSZ=1280

# ----- Debug -----
DEBUG=false
